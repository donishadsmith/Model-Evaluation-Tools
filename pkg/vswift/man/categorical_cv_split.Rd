% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/categorical_cv_split.R
\name{categorical_cv_split}
\alias{categorical_cv_split}
\title{Perform Train-Test Split and/or K-Fold Cross-Validation with optional stratified sampling for classification data}
\usage{
categorical_cv_split(
  data = NULL,
  target = NULL,
  predictors = NULL,
  split = NULL,
  n_folds = NULL,
  model_type = NULL,
  threshold = 0.5,
  stratified = FALSE,
  random_seed = NULL,
  remove_obs = FALSE,
  save_models = FALSE,
  save_data = FALSE,
  final_model = FALSE,
  ...
)
}
\arguments{
\item{data}{A data frame containing the dataset.}

\item{target}{The target variable's numerical index or name in the data frame.}

\item{predictors}{A vector of numerical indices or names for the predictors in the data frame.
If not specified, all variables except the response variable will be used as predictors.}

\item{split}{A number from 0.5 and 0.9 for the proportion of data to use for the training set,
leaving the rest for the test set. If not specified, train-test splitting will not be done.}

\item{n_folds}{An integer from 3 and 30 for the number of folds to use. If not specified,
k-fold cross validation will not be performed.}

\item{model_type}{A character string indicating the classification algorithm to use. Available options:
"lda", "qda", "logistic", "svm", "naivebayes", "ann", "knn", "decisiontree", "randomforest".
For "knn", the optimal k will be used unless specified with `ks =`.
For "ann", `size =` must be specified as an additional argument.}

\item{threshold}{A number from 0.3 to 0.7 indicating representing the decision boundary for logistic regression.}

\item{stratified}{A logical value indicating if stratified sampling should be used. Default = FALSE.}

\item{random_seed}{A numerical value for the random seed. Default is NULL.}

\item{remove_obs}{A logical value to remove observations with categorical predictors from the test/validation set
that have not been observed during model training. Some algorithms may produce an error if this occurs. Default is FALSE.}

\item{save_models}{A logical value to save models during train-test splitting and/or k-fold cross validation. Default is FALSE.}

\item{save_data}{A logical value to save all training and test/validation sets during train-test splitting and/or k-fold cross validation. Default is FALSE.}

\item{final_model}{A logical value to use all complete observations in the input data for model training. Default = FALSE.}

\item{...}{Additional arguments specific to the chosen classification algorithm.
Please refer to the corresponding algorithm's documentation for additional arguments and their descriptions.}
}
\value{
A list containing the results of train-test splitting and/or k-fold cross-validation,
        including performance metrics, information on the class distribution in the training, test sets, and folds (if applicable), 
        saved models (if specified), and saved datasets (if specified), and a final model (if specified).
}
\description{
`categorical_cv_split` performs a train-test split and/or k-fold cross validation
on classification data using various classification algorithms.
}
\section{Model-specific additional arguments}{

  Each model type accepts additional arguments specific to the classification algorithm. The available arguments for each model type are:

  - "lda": grouping, prior, method, nu
  - "qda": grouping, prior, method, nu
  - "logistic": weights, start, etastart, mustart, offset, control, contrasts, intercept, singular.ok, type
  - "svm": scale, type, kernel, degree, gamma, coef0, cost, nu, class.weights, cachesize, tolerance, epsilon, shrinking, cross, probability, fitted
  - "naivebayes": prior, laplace, usekernel, usepoisson
  - "ann": weights, size, Wts, mask, linout, entropy, softmax, censored, skip, rang, decay, maxit, Hess, trace, MaxNWts, abstol, reltol
  - "knn": kmax, ks, distance, kernel, scale, contrasts, ykernel
  - "decisiontree": weights, method, parms, control, cost
  - "randomforest": ntree, mtry, weights, replace, classwt, cutoff, strata, nodesize, maxnodes, importance, localImp, nPerm, proximity, oob.prox, norm.votes, do.trace, keep.forest, corr.bias, keep.inbag
}

\section{Functions used from packages for each model type}{


  - "lda": lda() from MASS package
  - "qda": qda() from MASS package
  - "logistic": glm() from base package with family = "binomial"
  - "svm": svm() from e1071 package
  - "naivebayes": naive_bayes() from naivebayes package
  - "ann": nnet() from nnet package
  - "knn": train.kknn() from kknn package
  - "decisiontree": rpart() from rpart package
  - "randomforest": randomForest() from randomForest package
}

\examples{
# Load an example dataset
data(iris)

# Perform a train-test split with an 80\% training set using LDA
result <- categorical_cv_split(data = iris, target = "Species", split = 0.8, model_type = "lda")

# Print parameters and metrics
print(result)

# Plot metrics
plot(result)

# Perform 5-fold cross-validation using QDA
result <- categorical_cv_split(data = iris, target = "Species", n_folds = 5, model_type = "qda")

# Print parameters and metrics
print(result)

# Plot metrics
plot(result)

}
\seealso{
\code{\link{print.vswift}}, \code{\link{plot.vswift}}
}
