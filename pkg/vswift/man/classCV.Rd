% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classCV.R
\name{classCV}
\alias{classCV}
\title{Perform Train-Test Split and/or K-Fold Cross-Validation with optional stratified sampling for classification data}
\usage{
classCV(
  formula = NULL,
  target = NULL,
  predictors = NULL,
  data,
  split = NULL,
  n_folds = NULL,
  model_type,
  threshold = 0.5,
  stratified = FALSE,
  random_seed = NULL,
  impute_method = NULL,
  impute_args = NULL,
  mod_args = NULL,
  remove_obs = FALSE,
  save_models = FALSE,
  save_data = FALSE,
  final_model = FALSE,
  n_cores = NULL,
  standardize = NULL,
  ...
)
}
\arguments{
\item{formula}{A formula specifying the model to use.}

\item{target}{The target variable's numerical index or name in the data frame.}

\item{predictors}{A vector of numerical indices or names for the predictors in the data frame.
If not specified, all variables except the response variable will be used as predictors.}

\item{data}{A data frame containing the dataset.}

\item{split}{A number from 0.5 and 0.9 for the proportion of data to use for the training set,
leaving the rest for the test set. If not specified, train-test splitting will not be done.}

\item{n_folds}{An integer from 3 and 30 for the number of folds to use. If not specified,
k-fold cross validation will not be performed.}

\item{model_type}{A character string or list indicating the classification algorithm to use. Available options:
"lda" (Linear Discriminant Analysis), "qda" (Quadratic Discriminant Analysis), 
"logistic" (Logistic Regression), "svm" (Support Vector Machines), "naivebayes" (Naive Bayes), 
"ann" (Artificial Neural Network), "knn" (K-Nearest Neighbors), "decisiontree" (Decision Tree), 
"randomforest" (Random Forest), "multinom" (Multinomial Logistic Regression), "gbm" (Gradient Boosting Machine).
For "knn", the optimal k will be used unless specified with `ks =`.
For "ann", `size =` must be specified as an additional argument.}

\item{threshold}{A number from 0.3 to 0.7 indicating representing the decision boundary for logistic regression.}

\item{stratified}{A logical value indicating if stratified sampling should be used. Default = FALSE.}

\item{random_seed}{A numerical value for the random seed. Default = NULL.}

\item{impute_method}{A character indicating the imputation method to use. Options include "bag_impute" (Bagged Trees Imputation) and "knn_impute" (KNN Imputation).}

\item{impute_args}{A list specifying an additional argument for the imputation method. For "bag_impute", the additional argument is "trees" and for "knn_impute", the additional argument is "neighbors". For specific information about each parameter, please refer to the recipes documentation. Default = NULL.}

\item{mod_args}{list of named sub-lists. Each sub-list corresponds to a model specified in the `model_type` parameter, and contains the parameters to be passed 
to the respective model. Default = NULL.}

\item{remove_obs}{A logical value to remove observations with categorical predictors from the test/validation set
that have not been observed during model training. Some algorithms may produce an error if this occurs. Default = FALSE.}

\item{save_models}{A logical value to save models during train-test splitting and/or k-fold cross validation. Default = FALSE.}

\item{save_data}{A logical value to save all training and test/validation sets during train-test splitting and/or k-fold cross validation. Default = FALSE.}

\item{final_model}{A logical value to use all complete observations in the input data for model training. Default = FALSE.}

\item{n_cores}{A numerical value specifying the number of cores to use for parallel processing. Default = NULL.}

\item{standardize}{A logical value or numerical vector. If TRUE, all columns except the target, columns of class character, and columns of class factor, will be standardized. To specify the columns to be standardized, create a numerical or character vector consisting of the column indices or names to be standardized.}

\item{...}{Additional arguments specific to the chosen classification algorithm.
Please refer to the corresponding algorithm's documentation for additional arguments and their descriptions.}
}
\value{
A list containing the results of train-test splitting and/or k-fold cross-validation (if specified), performance metrics, information on the class distribution in the training, test sets, and folds (if applicable), 
        saved models (if specified), and saved datasets (if specified), and a final model (if specified).
}
\description{
`classCV` performs a train-test split and/or k-fold cross validation
on classification data using various classification algorithms.
}
\section{Model-specific additional arguments}{

  Each model type accepts additional arguments specific to the classification algorithm. The available arguments for each model type are:

  - "lda": prior, method, nu
  - "qda": prior, method, nu
  - "logistic": weights, singular.ok, maxit
  - "svm": kernel, degree, gamma, cost, nu
  - "naivebayes": prior, laplace, usekernel
  - "ann": size, rang, decay, maxit, softmax
  - "knn": kmax, ks, distance, kernel
  - "decisiontree": weights, method, parms, control, cost
  - "randomforest": weights, ntree, mtry, nodesize, importance
  - "multinom": weights, Hess
  - "gbm": params, nrounds
}

\section{Functions used from packages for each model type}{


  - "lda": lda() from MASS package
  - "qda": qda() from MASS package
  - "logistic": glm() from base package with family = "binomial"
  - "svm": svm() from e1071 package
  - "naivebayes": naive_bayes() from naivebayes package
  - "ann": nnet() from nnet package
  - "knn": train.kknn() from kknn package
  - "decisiontree": rpart() from rpart package
  - "randomforest": randomForest() from randomForest package 
  - "multinom": multinom() from nnet package
  - "gbm": xgb.train() from xgboost package
}

\examples{
# Load an example dataset
data(iris)

# Perform a train-test split with an 80\% training set using LDA
result <- classCV(data = iris, target = "Species", 
                  split = 0.8, model_type = "lda")

# Print parameters and metrics
print(result)

# Plot metrics
plot(result)

# Perform 5-fold cross-validation using Gradient Boosted Model
result <- classCV(data = iris, target = "Species", n_folds = 5, 
                  model_type = "gbm",
                  params = list(objective = "multi:softprob",
                                num_class = 3,eta = 0.3,max_depth = 6), 
                                nrounds = 10)

# Print parameters and metrics
print(result)

# Plot metrics
plot(result)

# Perform 5-fold cross-validation a train-test split w/multiple models

args <- list("knn" = list(ks = 5), "ann" = list(size = 20))
result <- classCV(data = iris, target = 5, split = 0.8, 
                  model_type = c("decisiontree","knn", "ann","svm"), 
                  n_folds = 3,mod_args = args, stratified = TRUE)

# Print parameters and metrics
print(result)

# Plot metrics
plot(result)
}
\seealso{
\code{\link{print.vswift}}, \code{\link{plot.vswift}}
}
\author{
Donisha Smith
}
