% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classCV.R
\name{classCV}
\alias{classCV}
\title{Perform Train-Test Split and/or K-Fold Cross-Validation with optional stratified sampling for classification data}
\usage{
classCV(
  data,
  formula = NULL,
  target = NULL,
  predictors = NULL,
  model_type,
  threshold = 0.5,
  mod_args = NULL,
  final_model = FALSE,
  split = NULL,
  n_folds = NULL,
  stratified = FALSE,
  random_seed = NULL,
  impute_method = NULL,
  impute_args = NULL,
  save_models = FALSE,
  save_data = FALSE,
  n_cores = NULL,
  remove_obs = FALSE,
  standardize = NULL,
  ...
)
}
\arguments{
\item{data}{A data frame containing the dataset. Default = \code{NULL}}

\item{formula}{A formula specifying the model to use. Default = \code{NULL}}

\item{target}{The target variable's numerical index or name in the data frame. Default = \code{NULL}.}

\item{predictors}{A vector of numerical indices or names for the predictors in the data frame. Default = \code{NULL}.
If not specified, all variables except the response variable will be used as predictors.
Default = \code{NULL}.}

\item{model_type}{A character string or list indicating the classification algorithm to use. Available options:
\code{"lda"} (Linear Discriminant Analysis), \code{"qda"} (Quadratic Discriminant Analysis), 
\code{"logistic"} (Logistic Regression), \code{"svm"} (Support Vector Machines),
\code{"naivebayes"} (Naive Bayes), \code{"ann"} (Artificial Neural Network), \code{"knn"}
(K-Nearest Neighbors), \code{"decisiontree"} (Decision Tree), \code{"randomforest"} (Random Forest),
\code{"multinom"} (Multinomial Logistic Regression), \code{"gbm"} (Gradient Boosting Machine).
\itemize{
 \item For \code{"knn"}, the optimal k will be used unless specified with \code{ks}.
 \item For \code{"ann"}, \code{size} must be specified as an additional argument.
}}

\item{threshold}{A number from 0.3 to 0.7 indicating representing the decision boundary for logistic regression.
Default = \code{0.5}}

\item{mod_args}{list of named sub-lists. Each sub-list corresponds to a model specified in the \code{model_type}}

\item{final_model}{A logical value to use all complete observations in the input data for model training.
Default = \code{FALSE}.}

\item{split}{A number from 0.5 and 0.9 for the proportion of data to use for the training set,
leaving the rest for the test set. If not specified, train-test splitting will not be done.
Default = \code{NULL}.}

\item{n_folds}{An integer from 3 and 30 for the number of folds to use. If not specified,
k-fold cross validation will not be performed. Default = \code{NULL}.}

\item{stratified}{A logical value indicating if stratified sampling should be used. Default = \code{FALSE}.}

\item{random_seed}{A numerical value for the random seed to ensure random splitting is reproducible.
Default = \code{NULL}.}

\item{impute_method}{A character indicating the imputation method to use. Options include \code{"bag_impute"}
(Bagged Trees Imputation) and \code{"knn_impute"} (KNN Imputation).}

\item{impute_args}{A list specifying an additional argument for the imputation method. Below are the additional
  arguments available for each imputation option.
  \itemize{
    \item \code{"bag_impute"}: \code{trees}
    \item \code{"knn_impute"}: \code{neighbors}
  }
  For specific information about each parameter, please refer to the recipes documentation.
  Default = \code{NULL}.
parameter, and contains the parameters to be passed to the respective model. Default = \code{NULL}.}

\item{save_models}{A logical value to save models during train-test splitting and/or k-fold cross validation.
Default = \code{FALSE}.}

\item{save_data}{A logical value to save all training and test/validation sets during train-test splitting
and/or k-fold cross validation. Default = \code{FALSE}.}

\item{n_cores}{A numerical value specifying the number of cores to use for parallel processing.
Default = \code{NULL}.}

\item{remove_obs}{A logical value to remove observations with categorical predictors from the test/validation set
that have not been observed during model training. Some algorithms may produce an error if this
occurs. Default = \code{FALSE}.}

\item{standardize}{A logical value or numerical vector. If \code{TRUE}, all columns except the target, that are
numeric, will be standardized. To specify the columns to be standardized, create a numerical
or character vector consisting of the column indices or names to be standardized.}

\item{...}{Additional arguments specific to the chosen classification algorithm.
Please refer to the corresponding algorithm's documentation for additional arguments and their
descriptions.}
}
\value{
A list containing the results of train-test splitting and/or k-fold cross-validation (if specified),
        performance metrics, information on the class distribution in the training, test sets, and folds
        (if applicable), saved models (if specified), and saved datasets (if specified), and a final model
        (if specified).
}
\description{
performs a train-test split and/or k-fold cross validation
on classification data using various classification algorithms.
}
\section{Model-specific additional arguments}{

  Each option of \code{model_type} accepts additional arguments specific to the classification algorithm. The
  available arguments for each \code{model_type} are:
  \itemize{
   \item \code{"lda"}: \code{prior}, \code{method}, \code{nu}
   \item \code{"qda"}: \code{prior}, \code{method}, \code{nu}
   \item \code{"logistic"}: \code{weights}, \code{singular.ok}, \code{maxit}
   \item \code{"svm"}: \code{kernel}, \code{degree}, \code{gamma}, \code{cost}, \code{nu}
   \item \code{"naivebayes"}: \code{prior}, \code{laplace}, \code{usekernel}
   \item \code{"ann"}: \code{size}, \code{rang}, \code{decay}, \code{maxit}, \code{softmax},
                       \code{entropy}, \code{abstol}, \code{reltol}
   \item \code{"knn"}: \code{kmax}, \code{ks}, \code{distance}, \code{kernel}
   \item \code{"decisiontree"}: \code{weights}, \code{method},\code{parms}, \code{control}, \code{cost}
   \item \code{"randomforest"}: \code{weights}, \code{ntree}, \code{mtry}, \code{nodesize}, \code{importance}
   \item \code{"multinom"}: \code{weights}, \code{Hess}
   \item \code{"gbm"}: \code{params}, \code{nrounds}
  }
}

\section{Functions used from packages for each option for \code{model_type}}{

  \itemize{
   \item \code{"lda"}: \code{lda()} from MASS package
   \item \code{"qda"}: \code{qda()} from MASS package
   \item \code{"logistic"}: \code{glm()} from base package with \code{family = "binomial"}
   \item \code{"svm"}: \code{svm()} from e1071 package
   \item \code{"naivebayes"}: \code{naive_bayes()} from naivebayes package
   \item \code{"ann"}: \code{nnet()} from nnet package
   \item \code{"knn"}: \code{train.kknn()} from kknn package
   \item \code{"decisiontree"}: \code{rpart()} from rpart package
   \item \code{"randomforest"}: \code{randomForest()} from randomForest package 
   \item \code{"multinom"}: \code{multinom()} from nnet package
   \item \code{"gbm"}: \code{xgb.train()} from xgboost package
  }
}

\examples{
# Load an example dataset
data(iris)

# Perform a train-test split with an 80\% training set using LDA
result <- classCV(data = iris, target = "Species", 
                  split = 0.8, model_type = "lda")

# Print parameters and metrics
result

# Perform 5-fold cross-validation using Gradient Boosted Model
result <- classCV(data = iris, formula = Species~., n_folds = 5, 
                  model_type = "gbm",
                  params = list(objective = "multi:softprob",
                                num_class = 3,eta = 0.3,max_depth = 6), 
                                nrounds = 10)

# Print parameters and metrics
result


# Perform 5-fold cross-validation a train-test split w/multiple models

args <- list("knn" = list(ks = 5), "ann" = list(size = 20))
result <- classCV(data = iris, target = 5, split = 0.8, 
                  model_type = c("decisiontree","knn", "ann","svm"), 
                  n_folds = 3,mod_args = args, stratified = TRUE)

# Print parameters and metrics
result

}
\seealso{
\code{\link{print.vswift}}, \code{\link{plot.vswift}}
}
\author{
Donisha Smith
}
